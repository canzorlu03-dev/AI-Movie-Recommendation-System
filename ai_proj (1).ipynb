{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d9461-6fa0-4020-9a69-ed79530deba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1: SETUP, BRAIN, AND DEBUG UI ---\n",
    "# This notebook cell defines a small content-based movie recommender system\n",
    "# built on top of the TMDB 5000 dataset. It includes data loading and\n",
    "# preprocessing steps, a \"brain\" class that computes vector representations\n",
    "# and similarity scores, and a minimal ipywidgets-based UI for interactive\n",
    "# querying. Comments are written in an academic and precise style and do not\n",
    "# alter program logic.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import difflib\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# File paths for the dataset CSVs. These should point to the TMDB 5000\n",
    "# movies and credits CSV files. TOP_N_DEFAULT defines the default number of\n",
    "# recommendations returned when the user does not request a specific count.\n",
    "MOVIES_FILE = 'tmdb_5000_movies.csv'\n",
    "CREDITS_FILE = 'tmdb_5000_credits.csv'\n",
    "TOP_N_DEFAULT = 5\n",
    "\n",
    "# CUSTOM_STOP_WORDS extends the default scikit-learn English stop word list\n",
    "# with domain-specific tokens (e.g., 'movie', 'film') that are uninformative\n",
    "# for content-based similarity in this dataset.\n",
    "CUSTOM_STOP_WORDS = list(ENGLISH_STOP_WORDS) + ['movie', 'movies', 'film', 'films', 'cinema']\n",
    "\n",
    "\n",
    "# --- 2. HELPERS ---\n",
    "# The following helper functions perform small, well-scoped parsing tasks on\n",
    "# the dataset columns which are stored as stringified JSON-like lists.\n",
    "\n",
    "def convert(text):\n",
    "    \"\"\"\n",
    "    Parse a string representation of a list of dictionaries and extract the\n",
    "    'name' field from each dictionary. If parsing fails, return an empty list.\n",
    "    Typical input: \"[{'id': 28, 'name': 'Action'}, ...]\".\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    try:\n",
    "        for i in ast.literal_eval(text):\n",
    "            L.append(i['name'])\n",
    "    except: pass\n",
    "    return L\n",
    "\n",
    "\n",
    "def convert3(text):\n",
    "    \"\"\"\n",
    "    Similar to convert(), but limits the output to at most eight names.\n",
    "    This is used to restrict cast lists so that extremely large casts do not\n",
    "    dominate the text representation.\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    counter = 0\n",
    "    try:\n",
    "        for i in ast.literal_eval(text):\n",
    "            if counter < 8:\n",
    "                L.append(i['name'])\n",
    "                counter+=1\n",
    "            else: break\n",
    "    except: pass\n",
    "    return L\n",
    "\n",
    "\n",
    "def fetch_director(text):\n",
    "    \"\"\"\n",
    "    Extract the director's name from a crew list encoded as a string.\n",
    "    The function returns a single-element list containing the director's\n",
    "    name if present. Returning as a list keeps the downstream pipeline\n",
    "    consistent (other fields produce lists of names).\n",
    "    \"\"\"\n",
    "    L = []\n",
    "    try:\n",
    "        for i in ast.literal_eval(text):\n",
    "            if i['job'] == 'Director':\n",
    "                L.append(i['name'])\n",
    "                break\n",
    "    except: pass\n",
    "    return L\n",
    "\n",
    "\n",
    "def collapse(L):\n",
    "    \"\"\"\n",
    "    Helper to remove whitespace within tokens. Many downstream steps use\n",
    "    space-free tokens (e.g., 'Tom Hanks' -> 'TomHanks') to create compact\n",
    "    vocabulary items which are robust to tokenization differences.\n",
    "    \"\"\"\n",
    "    return [i.replace(\" \",\"\") for i in L]\n",
    "\n",
    "\n",
    "# --- 3. THE BRAIN (Level 8 with Debug Logs) ---\n",
    "# The UniversalRecommender class encapsulates all logic for building the\n",
    "# content-based recommendation model and for producing recommendations given\n",
    "# a text query. Implementation notes:\n",
    "# - Uses a simple CountVectorizer over concatenated textual 'tags'.\n",
    "# - Combines cosine similarity with a normalized popularity-weighted score\n",
    "#   (a Bayes-like weighted rating) for final ranking.\n",
    "# - Includes a sliding-window parser and fuzzy matching to robustly map\n",
    "#   user queries to vocabulary tokens.\n",
    "class UniversalRecommender:\n",
    "    def __init__(self):\n",
    "        # Initialization message to indicate model-building activity.\n",
    "        print(\"‚öôÔ∏è Initializing AI Brain... (10-20 seconds)\")\n",
    "        # Validate dataset presence; fail fast if files are missing.\n",
    "        if not os.path.exists(MOVIES_FILE) or not os.path.exists(CREDITS_FILE):\n",
    "            print(f\"‚ùå Error: Files not found.\")\n",
    "            return\n",
    "\n",
    "        # Load CSV files and merge on the 'title' column to create a single\n",
    "        # dataframe with both movie metadata and credits information.\n",
    "        movies = pd.read_csv(MOVIES_FILE)\n",
    "        credits = pd.read_csv(CREDITS_FILE)\n",
    "        self.df = movies.merge(credits, on='title')\n",
    "\n",
    "        # Select the subset of columns required by the recommender pipeline.\n",
    "        self.df = self.df[['movie_id', 'title', 'overview', 'genres', 'keywords', 'cast', 'crew', 'vote_average', 'vote_count', 'popularity']]\n",
    "        \n",
    "        # Parse JSON-like genre strings into Python lists of genre names.\n",
    "        self.df['genres'] = self.df['genres'].apply(convert)\n",
    "\n",
    "        # Build a set of known genres (lowercased) used for quick detection of\n",
    "        # genre-based user queries. This enables direct genre searches.\n",
    "        self.known_genres = set()\n",
    "        for g_list in self.df['genres']:\n",
    "            for g in g_list:\n",
    "                self.known_genres.add(g.lower())\n",
    "        # Add common alternate genre synonyms to increase robustness.\n",
    "        self.known_genres.update(['science fiction', 'sci-fi', 'rom-com'])\n",
    "\n",
    "        # Parse keywords, cast, and crew using the helper functions defined\n",
    "        # above. These produce lists of textual tokens used later in tag\n",
    "        # construction.\n",
    "        self.df['keywords'] = self.df['keywords'].apply(convert)\n",
    "        self.df['cast'] = self.df['cast'].apply(convert3)\n",
    "        self.df['crew'] = self.df['crew'].apply(fetch_director)\n",
    "\n",
    "        # A user-friendly string showing genres for display in the UI.\n",
    "        self.df['display_genres'] = self.df['genres'].apply(lambda x: \", \".join(x))\n",
    "\n",
    "        # --- Weighted rating computation ---\n",
    "        # Compute an aggregate 'weighted_score' that accounts for both the\n",
    "        # average rating and the number of votes. This mirrors the IMDB-style\n",
    "        # Bayesian estimate where movies with very few votes are penalized.\n",
    "        C = self.df['vote_average'].mean()\n",
    "        m = self.df['vote_count'].quantile(0.70)\n",
    "        def weighted_rating(x, m=m, C=C):\n",
    "            v = x['vote_count']\n",
    "            R = x['vote_average']\n",
    "            if v >= m: return (v/(v+m) * R) + (m/(m+v) * C)\n",
    "            else: return 0\n",
    "\n",
    "        self.df['weighted_score'] = self.df.apply(weighted_rating, axis=1)\n",
    "\n",
    "        # Normalize the weighted score to the [0,1] interval so it can be\n",
    "        # mixed with similarity scores during ranking.\n",
    "        scaler = MinMaxScaler()\n",
    "        self.df['normalized_weight'] = scaler.fit_transform(self.df[['weighted_score']])\n",
    "\n",
    "        # Clean and collapse tokens for consistent tokenization in the vector\n",
    "        # space. This removes spaces inside multi-word names, e.g., 'Tom Hanks'\n",
    "        # -> 'TomHanks', which ensures they are treated as single tokens.\n",
    "        self.df['cast_clean'] = self.df['cast'].apply(collapse)\n",
    "        self.df['crew_clean'] = self.df['crew'].apply(collapse)\n",
    "        self.df['genres_clean'] = self.df['genres'].apply(collapse)\n",
    "        self.df['keywords_clean'] = self.df['keywords'].apply(collapse)\n",
    "\n",
    "        # Tokenize the overview into words (simple whitespace tokenization).\n",
    "        # If the overview is missing or not a string, use an empty list.\n",
    "        self.df['overview_clean'] = self.df['overview'].apply(lambda x: x.split() if isinstance(x, str) else [])\n",
    "\n",
    "        # Construct a compact 'tags' field used as the textual representation\n",
    "        # for each movie. The design choices here intentionally weight certain\n",
    "        # fields (genres and cast) more heavily by duplicating them.\n",
    "        self.df['tags'] = self.df['overview_clean'] + (self.df['genres_clean'] * 2) + self.df['keywords_clean'] + (self.df['cast_clean'] * 2) + self.df['crew_clean']\n",
    "        self.df['tags'] = self.df['tags'].apply(lambda x: \" \".join(x))\n",
    "        \n",
    "        # Initialize the CountVectorizer using the extended stop-word list.\n",
    "        # The resulting vocabulary will be used for fast BoW vectorization.\n",
    "        self.cv = CountVectorizer(stop_words=CUSTOM_STOP_WORDS) \n",
    "        self.vectors = self.cv.fit_transform(self.df['tags']).toarray()\n",
    "        self.vocab = list(self.cv.vocabulary_.keys())\n",
    "        print(f\"‚úÖ Model Ready. Learned {len(self.vocab)} unique tags.\")\n",
    "\n",
    "    def recommend(self, query: str, top_n: int = TOP_N_DEFAULT):\n",
    "        \"\"\"\n",
    "        Produce up to top_n movie recommendations for an arbitrary textual\n",
    "        query. Returns a tuple: (results, logs). 'results' is a list of\n",
    "        recommendation dicts; 'logs' is a sequence of debug strings that\n",
    "        describe how the query was interpreted.\n",
    "\n",
    "        The method implements a robust pipeline that:\n",
    "        - Detects explicit genres and exact titles\n",
    "        - Applies fuzzy matching to fix typos or map user words to vocabulary\n",
    "        - Uses a sliding-window (trigram/bigram/unigram) parser to detect\n",
    "          multi-word entities (e.g., actor names)\n",
    "        - Computes cosine similarity in BoW space and combines it with the\n",
    "          normalized popularity-weight to produce final scores.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_lower = query.lower().strip()\n",
    "            glued_query = query_lower.replace(\" \",\"\")\n",
    "            logs = [] # Store debug messages here\n",
    "            \n",
    "            # --- GENRE CHECK ---\n",
    "            # If the user query exactly matches a known genre (or a cleaned\n",
    "            # variant), we record that fact in the logs and allow downstream\n",
    "            # processing (the actual genre-handling logic may be expanded\n",
    "            # here in future revisions).\n",
    "            clean_check = query_lower.replace(\"movies\", \"\").replace(\"movie\", \"\").strip()\n",
    "            if query_lower in self.known_genres or clean_check in self.known_genres:\n",
    "                logs.append(f\"üîé Genre Detected: '{clean_check}'\")\n",
    "            else:\n",
    "                # Build a mapping from lowercased titles to dataframe indices\n",
    "                # for efficient exact-title detection.\n",
    "                title_map = pd.Series(self.df.index, index=self.df['title'].apply(lambda x: x.lower()))\n",
    "                \n",
    "                # 1. Exact Title: If user provided a precise title, return\n",
    "                # recommendations based on content-similarity to that movie.\n",
    "                if query_lower in title_map:\n",
    "                    idx = title_map[query_lower]\n",
    "                    if isinstance(idx, pd.Series): idx = idx.iloc[0]\n",
    "                    return self._fetch_results(idx, query_lower, top_n), logs\n",
    "\n",
    "                # 2. Known Entity: If the concatenated query exists in the\n",
    "                # vocabulary (e.g., 'tomhanks'), treat it as a recognized token.\n",
    "                if glued_query in self.vocab:\n",
    "                    logs.append(f\"üåü Known Entity: '{glued_query}'\")\n",
    "                else:\n",
    "                    # 3. Fuzzy Title: Attempt to find a close movie title using\n",
    "                    # difflib; this helps when the user makes minor spelling\n",
    "                    # errors but intends a movie title.\n",
    "                    all_titles = self.df['title'].tolist()\n",
    "                    closest_titles = difflib.get_close_matches(query, all_titles, n=1, cutoff=0.85)\n",
    "                    if closest_titles:\n",
    "                        match = closest_titles[0]\n",
    "                        if match.lower() not in self.known_genres:\n",
    "                            logs.append(f\"üîÑ Fuzzy Title: '{match}'\")\n",
    "                            match_lower = match.lower()\n",
    "                            if match_lower in title_map:\n",
    "                                idx = title_map[match_lower]\n",
    "                                if isinstance(idx, pd.Series): idx = idx.iloc[0]\n",
    "                                return self._fetch_results(idx, match_lower, top_n), logs\n",
    "\n",
    "            # --- SLIDING WINDOW PARSER ---\n",
    "            # The sliding window attempts to detect multi-word vocabulary items\n",
    "            # by concatenating adjacent words (trigram, bigram) and checking\n",
    "            # for close matches in the pre-computed vocabulary. This increases\n",
    "            # robustness to names like 'keanu reeves' or 'al pacino'.\n",
    "            parts = query_lower.split()\n",
    "            final_search_terms = []\n",
    "            i = 0\n",
    "            while i < len(parts):\n",
    "                # Try trigram detection first (3-word sequences). We use a high\n",
    "                # cutoff to avoid false positives.\n",
    "                if i + 2 < len(parts):\n",
    "                    trigram = parts[i] + parts[i+1] + parts[i+2]\n",
    "                    closest = difflib.get_close_matches(trigram, self.vocab, n=1, cutoff=0.9)\n",
    "                    if closest:\n",
    "                        logs.append(f\"üß© Detected Actor (3-word): {closest[0]}\")\n",
    "                        final_search_terms.extend([closest[0]] * 3); i += 3; continue\n",
    "                # Next, attempt bigram detection (2-word sequences).\n",
    "                if i + 1 < len(parts):\n",
    "                    bigram = parts[i] + parts[i+1]\n",
    "                    closest = difflib.get_close_matches(bigram, self.vocab, n=1, cutoff=0.9)\n",
    "                    if closest:\n",
    "                        logs.append(f\"üß© Detected Actor (2-word): {closest[0]}\")\n",
    "                        final_search_terms.extend([closest[0]] * 3); i += 2; continue\n",
    "                    # Allow looser fuzzy matching for bigrams to correct simple typos.\n",
    "                    closest_fuzzy = difflib.get_close_matches(bigram, self.vocab, n=1, cutoff=0.7)\n",
    "                    if closest_fuzzy:\n",
    "                        logs.append(f\"üîß Fixed Bigram Typo: '{bigram}' -> '{closest_fuzzy[0]}'\")\n",
    "                        final_search_terms.extend([closest_fuzzy[0]] * 3); i += 2; continue\n",
    "\n",
    "                # Fall back to unigram processing. If the word exists in the\n",
    "                # vocabulary, include it; if it looks like a stop word, skip it;\n",
    "                # otherwise attempt a fuzzy correction.\n",
    "                word = parts[i]\n",
    "                if word in self.vocab: final_search_terms.append(word)\n",
    "                elif word in CUSTOM_STOP_WORDS: pass\n",
    "                else:\n",
    "                    closest = difflib.get_close_matches(word, self.vocab, n=1, cutoff=0.6)\n",
    "                    if closest: \n",
    "                        logs.append(f\"üîß Fixed Typo: {word} -> {closest[0]}\")\n",
    "                        final_search_terms.append(closest[0])\n",
    "                i += 1\n",
    "            \n",
    "            combined_query = \" \".join(final_search_terms)\n",
    "            \n",
    "            # --- SHOW THE FINAL VECTOR ---\n",
    "            logs.append(f\"üìù Final Vector: '{combined_query}'\")\n",
    "            \n",
    "            # If query reduces to an empty vector after cleaning, return empty\n",
    "            # results and the log explaining why.\n",
    "            if not combined_query.strip(): return [], logs\n",
    "\n",
    "            # Transform the combined query into the bag-of-words space and\n",
    "            # compute cosine similarity against all movie tag vectors.\n",
    "            search_vector = self.cv.transform([combined_query])\n",
    "            sim_scores = cosine_similarity(search_vector, self.vectors).flatten()\n",
    "            pop_scores = self.df['normalized_weight'].values\n",
    "            \n",
    "            # Final score is a weighted combination of content similarity and\n",
    "            # normalized popularity. The (sim_scores > 0) multiplier zeroes\n",
    "            # out movies that have no token overlap with the query.\n",
    "            final_scores = (sim_scores * 0.8) + (pop_scores * 0.2)\n",
    "            final_scores = final_scores * (sim_scores > 0)\n",
    "\n",
    "            sorted_indices = final_scores.argsort()[::-1]\n",
    "            \n",
    "            # Collect top-n results where final score is positive.\n",
    "            results = []\n",
    "            count = 0\n",
    "            for idx in sorted_indices:\n",
    "                if final_scores[idx] > 0:\n",
    "                    results.append({\n",
    "                        'title': self.df.iloc[idx].title,\n",
    "                        'genres': self.df.iloc[idx].display_genres,\n",
    "                        'score': float(final_scores[idx])\n",
    "                    })\n",
    "                    count += 1\n",
    "                if count >= top_n: break\n",
    "            return results, logs\n",
    "\n",
    "        except Exception as e:\n",
    "            # On exception, return an empty result list and a single-element\n",
    "            # log describing the error for debugging in interactive use.\n",
    "            return [], [f\"Error: {e}\"]\n",
    "\n",
    "    def _fetch_results(self, idx, exclude_title, top_n):\n",
    "        \"\"\"\n",
    "        Given a movie index, compute the most similar movies based on the\n",
    "        precomputed tag vectors. The exclude_title parameter avoids\n",
    "        recommending the same movie as the user query.\n",
    "        \"\"\"\n",
    "        search_vector = self.vectors[idx].reshape(1, -1)\n",
    "        sim_scores = cosine_similarity(search_vector, self.vectors).flatten()\n",
    "        sorted_indices = sim_scores.argsort()[::-1]\n",
    "        results = []\n",
    "        count = 0\n",
    "        for i in sorted_indices:\n",
    "            if self.df.iloc[i].title.lower() == exclude_title: continue\n",
    "            results.append({\n",
    "                'title': self.df.iloc[i].title,\n",
    "                'genres': self.df.iloc[i].display_genres,\n",
    "                'score': float(sim_scores[i])\n",
    "            })\n",
    "            count += 1\n",
    "            if count >= top_n: break\n",
    "        return results\n",
    "\n",
    "\n",
    "# --- 4. INITIALIZE ---\n",
    "# Construct the recommender instance. This will load and preprocess the data\n",
    "# and print a ready message when complete. The printed message helps the user\n",
    "# understand that the potentially long initialization step finished.\n",
    "bot = UniversalRecommender()\n",
    "\n",
    "\n",
    "# --- 5. JUPYTER UI ---\n",
    "# Create a minimal interactive UI using ipywidgets. The UI comprises a text\n",
    "# input box, a search button, and an output area. The run_search callback\n",
    "# invokes the recommender and renders both debug logs and results.\n",
    "input_box = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Try: Keenu Reaves, Al Pachino, Sci-Fi...',\n",
    "    description='Search:',\n",
    "    layout=widgets.Layout(width='600px'),\n",
    "    continuous_update=False\n",
    ")\n",
    "search_btn = widgets.Button(description=\"Recommend üçø\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def run_search(_ignore=None):\n",
    "    \"\"\"\n",
    "    Callback to handle search events. The function prints a small progress\n",
    "    message, calls the recommend() method, then clears and displays the\n",
    "    debug log and the top recommendations in a human-readable form.\n",
    "    \"\"\"\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        query = input_box.value\n",
    "        if not query: return\n",
    "        \n",
    "        print(f\"Thinking about '{query}'...\")\n",
    "        results, logs = bot.recommend(query, top_n=5)\n",
    "        \n",
    "        output_area.clear_output()\n",
    "        \n",
    "        # --- DISPLAY DEBUG LOGS ---\n",
    "        if logs:\n",
    "            display(Markdown(\"**ü§ñ AI Debug Log:**\"))\n",
    "            for log in logs:\n",
    "                print(log) # This prints the vector and typos\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        # --- DISPLAY RESULTS ---\n",
    "        if not results:\n",
    "            print(f\"‚ùå No matches found for '{query}'.\")\n",
    "        else:\n",
    "            display(Markdown(f\"### Top Recommendations for **{query}**:\"))\n",
    "            for r in results:\n",
    "                # Create a simple ASCII progress bar to visualize the score.\n",
    "                score_bar = \"‚ñì\" * int(r['score']*20) + \"‚ñë\" * (20 - int(r['score']*20))\n",
    "                display(Markdown(f\"**üé¨ {r['title']}**\"))\n",
    "                print(f\"Genres: {r['genres']}\")\n",
    "                print(f\"Match:  {int(r['score']*100)}% | {score_bar}\")\n",
    "                print(\"\\n\")\n",
    "\n",
    "# Wire UI controls to the callback functions and display them in the\n",
    "# notebook. The input box also triggers searches when its value changes.\n",
    "search_btn.on_click(run_search)\n",
    "input_box.observe(run_search, names='value')\n",
    "\n",
    "# Render the UI controls and the output area for interactive use.\n",
    "display(widgets.HBox([input_box, search_btn]))\n",
    "display(output_area)\n",
    "\n",
    "# End of annotated module. The code and logic above remain unchanged; only\n",
    "# explanatory comments have been added to aid readability and maintainability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857521f7-c737-4e9b-bb21-211b515f84f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
